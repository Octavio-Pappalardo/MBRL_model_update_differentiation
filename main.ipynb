{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import ray\n",
    "\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchopt\n",
    "import wandb\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sac_v_data_collection import ReplayBuffer ,compute_loss_in_virtual_MDP , take_M_steps_in_env\n",
    "\n",
    "from sac_v_neural_nets import State_encoder_stochastic, State_encoder_deterministic, SquashedGaussianMLPActor , MLPQFunction , Model \n",
    "\n",
    "from sac_v_updates import distributed_sac_update ,update_model_target_networks ,update_agent_with_virtual_data_v1,update_agent_with_virtual_data_v2 \n",
    "\n",
    "from sac_test_and_logs import policy_evaluation , test_policy_with_adaptations\n",
    "\n",
    "\n",
    "from sac_v_config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_setting='custom'\n",
    "config=get_config(config_setting)\n",
    "wyb=True\n",
    "\n",
    "if config.seeding==True:\n",
    "    torch.manual_seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "\n",
    "device_real=config.device_real_world\n",
    "device_virtual=config.device_virtual_world\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_id='Hopper-v4' \n",
    "def make_env(env_id):\n",
    "    env = gym.make(env_id)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    if config.seeding==True:\n",
    "        env.action_space.seed(config.seed)\n",
    "        env.observation_space.seed(config.seed)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env, test_env = make_env(env_id), make_env(env_id)\n",
    "obs_dim = env.observation_space.shape\n",
    "act_dim = env.action_space.shape\n",
    "act_limit = env.action_space.high[0]\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, max_size=config.replay_buffer_size)\n",
    "\n",
    "\n",
    "# build policy and value functions\n",
    "if not config.stochastic_encoder:\n",
    "    state_encoder=State_encoder_deterministic(obs_dim=obs_dim[0] ,obs_encoding_size=config.virtual_state_dim)\n",
    "else:\n",
    "    state_encoder=State_encoder_stochastic(obs_dim=obs_dim[0] ,obs_encoding_size=config.virtual_state_dim)\n",
    "\n",
    "\n",
    "policy = SquashedGaussianMLPActor(input_dim=config.virtual_state_dim, act_dim=act_dim[0],\n",
    "                                    hidden_sizes=(256,256), activation=nn.ReLU, act_limit=act_limit) \n",
    "q1 = MLPQFunction(obs_dim=obs_dim[0], act_dim=act_dim[0],\n",
    "                    hidden_sizes=(256,256), activation=nn.ReLU)\n",
    "q2 = MLPQFunction(obs_dim=obs_dim[0], act_dim=act_dim[0],\n",
    "                    hidden_sizes=(256,256), activation=nn.ReLU)\n",
    "\n",
    "#target networks\n",
    "target_q1 =  deepcopy(q1)  \n",
    "target_q2 =  deepcopy(q2)  \n",
    "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "for p in target_q1.parameters(): \n",
    "    p.requires_grad = False  \n",
    "for p in target_q2.parameters(): \n",
    "    p.requires_grad = False  \n",
    "\n",
    "\n",
    "# List of parameters for both Q-networks\n",
    "q_params = list(q1.parameters()) + list(q2.parameters()) \n",
    "\n",
    "# Set up optimizers for policy and q-functions\n",
    "policy_optimizer = Adam(policy.parameters(), lr=config.policy_lr)\n",
    "qs_optimizer = Adam(q_params, lr=config.q_func_lr)\n",
    "state_encoder_optimizer=  Adam(state_encoder.parameters(), lr=config.encoder_lr)\n",
    "\n",
    "\n",
    "\n",
    "model=Model( virtual_state_dim=config.virtual_state_dim ,\n",
    "            env=env , len_virtual_trayectories=config.len_virtual_trayectories)\n",
    "\n",
    "model_optimizer= Adam(model.parameters(), lr=config.model_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.episodes_returns=[0]\n",
    "        self.episodes_lengths=[0]\n",
    "        self.num_episodes=0\n",
    "\n",
    "        #metrics during updates\n",
    "        self.num_updates=0\n",
    "        self.num_real_steps_at_time_of_update=[]\n",
    "        self.policy_loss = []\n",
    "        self.q_loss= []\n",
    "        self.actions_logprobs=[]\n",
    "        self.entropies=[]\n",
    "        self.virtual_loss=[0]\n",
    "        self.kl_regu_loss=[0]\n",
    "\n",
    "        self.q1_means= []\n",
    "        self.q1_stds = []\n",
    "        self.q2_means= []\n",
    "        self.q2_stds = []\n",
    "        self.model_consistency_loss=[]\n",
    "\n",
    "        #tests metrics\n",
    "        self.num_real_steps_at_time_of_test=[]\n",
    "        self.test_episodes_returns = []\n",
    "        self.test_episodes_lengths = []\n",
    "        self.test_base_params_episodes_returns=[]\n",
    "\n",
    "    def prepare_for_wyb_logging(self):\n",
    "        # define our custom x axis metric\n",
    "        wandb.define_metric(\"num updates\")\n",
    "        # define which metrics will be plotted against it\n",
    "        wandb.define_metric(\"q1 means\", step_metric=\"num updates\")\n",
    "        wandb.define_metric(\"q2 means\", step_metric=\"num updates\")\n",
    "        wandb.define_metric(\"q1 stds\", step_metric=\"num updates\")\n",
    "        wandb.define_metric(\"q2 stds\", step_metric=\"num updates\")\n",
    "        wandb.define_metric(\"policy loss\", step_metric=\"num updates\")\n",
    "        wandb.define_metric(\"model_consistency_loss\", step_metric=\"num updates\")\n",
    "        wandb.define_metric(\"q loss\", step_metric=\"num updates\")\n",
    "        wandb.define_metric(\"actions logprobs\", step_metric=\"num updates\")\n",
    "        wandb.define_metric(\"entropy\", step_metric=\"num updates\")\n",
    "        wandb.define_metric(\"virtual loss\", step_metric=\"num updates\")\n",
    "        wandb.define_metric(\"kl regularization loss\",step_metric=\"num updates\")\n",
    "        wandb.define_metric(\"real steps at time of update\", step_metric=\"num updates\")\n",
    "        \n",
    "\n",
    "    def log_update_metrics(self,total_real_steps):\n",
    "        self.num_updates+=1\n",
    "        wandb.log({'q1 means': self.q1_means[-1] ,'q2 means':self.q1_means[-1] ,\n",
    "                    'q1 stds': self.q1_stds[-1] ,'q2 stds':self.q2_stds[-1] , \n",
    "                    'policy loss': self.policy_loss[-1] , 'model_consistency_loss':self.model_consistency_loss[-1],\n",
    "                    'q loss':self.q_loss[-1] ,\n",
    "                    'actions logprobs': self.actions_logprobs[-1] , 'entropy': self.entropies[-1] ,\n",
    "                    'virtual loss':self.virtual_loss[-1] ,'kl regularization loss':self.kl_regu_loss[-1],\n",
    "                    'real steps at time of update': self.num_real_steps_at_time_of_update[-1],\n",
    "                     'num updates' : self.num_updates},step=total_real_steps)\n",
    "\n",
    "\n",
    "    def log_training_performance( self,total_real_steps):\n",
    "        if len(self.episodes_returns) > self.num_episodes:\n",
    "            wandb.log({'episodes returns': np.mean(self.episodes_returns[-1]),\n",
    "                    'episodes lengths':np.mean(self.episodes_lengths[-1]) } ,step=total_real_steps)\n",
    "        self.num_episodes= len(self.episodes_returns) \n",
    "            \n",
    "\n",
    "    def log_test_performance(self ,total_real_steps):\n",
    "        wandb.log({'test_base_params_episodes_returns': np.mean(self.test_base_params_episodes_returns[-1]),\n",
    "                'test_episodes_returns':np.mean(self.test_episodes_returns[-1]) ,\n",
    "                'test_episodes_lengths':np.mean(self.test_episodes_lengths[-1]),\n",
    "                 'real steps at time of test': self.num_real_steps_at_time_of_test[-1] } ,step=total_real_steps  )\n",
    "\n",
    "logger=Logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if wyb:\n",
    "    model_id=int(time.time())\n",
    "    run_name = f\"{env_id}__{model_id}\" \n",
    "    wandb.init(project='project_name',\n",
    "                    name= run_name,\n",
    "                    config=vars(config))\n",
    "    logger.prepare_for_wyb_logging()\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "\n",
    "c=0 #dummy variable for determining when to do an evaluation with a deterministic version of the policy and an evaluation of the base policy\n",
    "update_number=0\n",
    "\n",
    "\n",
    "new_state=torch.tensor(env.reset()[0],dtype=torch.float32).to(device_virtual) #get a first observation from environment\n",
    "\n",
    "total_performed_steps=0\n",
    "\n",
    "while total_performed_steps< config.total_timesteps:\n",
    "    #each loop makes use of [config.num_real_mdp_steps_per_update] new steps of real data\n",
    "    base_policy_state_dict = torchopt.extract_state_dict(policy)\n",
    "\n",
    "\n",
    "    ######## ############## ------ REAL MDP DATA COLLECTION ----------############## ##############\n",
    "    update_current_step_num=0\n",
    "\n",
    "    while (update_current_step_num + config.num_real_mdp_steps_per_adaptation)<config.num_real_mdp_steps_per_update:\n",
    "\n",
    "        #perofrm random actions in the first steps\n",
    "        if total_performed_steps < config.num_initial_random_steps:\n",
    "            new_state, num_steps_taken = take_M_steps_in_env(random_actions=True, state_encoder=state_encoder,policy=policy,env=env, replay_buffer=replay_buffer , \n",
    "                                        m=config.num_real_mdp_steps_per_adaptation ,current_state=new_state ,\n",
    "                                        finish_if_episode_ends=True, logger=logger,device=device_real)\n",
    "        \n",
    "        #adapt the base policy and perform actions with it for data collection\n",
    "        else:\n",
    "            adaptation_optimizer =torchopt.MetaSGD(policy, lr=config.adaptation_lr) \n",
    "            #adapt base policy using model data \n",
    "            for l in range(config.num_updates_in_adaptation):\n",
    "                policy_loss_for_adaptation=compute_loss_in_virtual_MDP(state_encoder=state_encoder,policy=policy, model=model , current_real_state=new_state ,\n",
    "                                                                        config=config,device=device_virtual)\n",
    "                adaptation_optimizer.step(policy_loss_for_adaptation)\n",
    "            #steps in real world - collect data with adapted policy in real world for m steps and add them to the replay_buffer . \n",
    "            new_state, num_steps_taken = take_M_steps_in_env(state_encoder=state_encoder,policy=policy,env=env, replay_buffer=replay_buffer , \n",
    "                                                        m=config.num_real_mdp_steps_per_adaptation ,current_state=new_state ,\n",
    "                                                        finish_if_episode_ends=True,logger=logger, device=device_real)\n",
    "            torchopt.recover_state_dict(policy, base_policy_state_dict)\n",
    "\n",
    "        update_current_step_num+=num_steps_taken\n",
    "        total_performed_steps+=num_steps_taken\n",
    "\n",
    "    print(f'return at {total_performed_steps} steps taken = {np.mean(logger.episodes_returns[-1:])}' )\n",
    "    if wyb:\n",
    "        logger.log_training_performance(total_real_steps=total_performed_steps)\n",
    "\n",
    "    ######## ############## ############## ############## ############## ##############\n",
    "\n",
    "\n",
    "\n",
    "    ######## ############## ------   UPDATE MODELS ---------- ############## ##############\n",
    "\n",
    "\n",
    "    start_time=time.time()\n",
    "    if total_performed_steps< config.num_steps_to_start_updating_after:\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        for j in range(update_current_step_num * config.updates_to_steps_ratio):  #perform as many update iterations as num of steps collected times a ration defined in config #range(config.num_real_mdp_steps_per_update):\n",
    "            \n",
    "\n",
    "            distributed_sac_update(replay_buffer =replay_buffer,state_encoder=state_encoder, policy=policy , model=model ,\n",
    "                    q1=q1 ,q2=q2 , target_q1=target_q1, target_q2=target_q2  , state_encoder_optimizer=state_encoder_optimizer,\n",
    "                    policy_optimizer=policy_optimizer ,qs_optimizer=qs_optimizer, model_optimizer=model_optimizer ,\n",
    "                        config=config , logger=logger)\n",
    "            \n",
    "            update_model_target_networks(model,config)\n",
    "\n",
    "            update_agent_with_virtual_data_v2(replay_buffer=replay_buffer , num_states_to_consider=config.num_states_for_estimating_virtual_loss,\n",
    "                                              state_encoder=state_encoder, policy=policy  , model=model , policy_optimizer=policy_optimizer ,config=config, logger=logger)\n",
    "            #update_agent_with_virtual_data_v1(replay_buffer=replay_buffer , num_states_to_consider=config.num_states_for_estimating_virtual_loss\n",
    "            #                                   , state_encoder=state_encoder, policy=policy  , model=model , policy_optimizer=policy_optimizer\n",
    "            #                                   , config=config, logger=logger)\n",
    "\n",
    "\n",
    "            if wyb:\n",
    "                logger.num_real_steps_at_time_of_update.append(total_performed_steps)\n",
    "                logger.log_update_metrics(total_real_steps=total_performed_steps)\n",
    "            update_number+=1\n",
    "\n",
    "    print(f'updating takes:{time.time()-start_time}')\n",
    "\n",
    "    if total_performed_steps//3000 > c:\n",
    "        logger.num_real_steps_at_time_of_test.append(total_performed_steps)\n",
    "        base_policy_returns=policy_evaluation(state_encoder=state_encoder,policy=policy, env=test_env, num_episodes=config.num_test_episodes, deterministic=True,device=device_real) \n",
    "        logger.test_base_params_episodes_returns.append(base_policy_returns)\n",
    "        test_policy_with_adaptations(state_encoder, policy ,model, env, num_episodes=config.num_test_episodes, config=config ,logger=logger )\n",
    "        if wyb:\n",
    "            logger.log_test_performance(total_real_steps=total_performed_steps)\n",
    "        c+=1\n",
    "        \n",
    "\n",
    "if wyb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
